# Hello DES INV 202 Student!
Welcome to your new GitHub repository! 

# Outline
[week 1](README.md#week-1-example-report-1)

week 2, etc...

---

# Github Background Information & Context
If you‚Äôre new to GitHub, you can think of this as a shared file space (like a Google Drive folder, or a like a USB drive that‚Äôs hosted online.) 

This is your space to store project files, videos, PDFs, notes, images, etc., and (hopefully, neatly) organize so it's easy for viewers (and you!) to navigate. That said, it‚Äôs super easy for you to share any file or folder with us (your TDF instructional team) - just send us the link!  As a start, feel free to simply add images to the `/assets` folder, which is located [here](/assets). 

The specific file that I‚Äôm typing into right now is the **README.md** for this repo. 
##### (üí° TIP: The .md indicates that we‚Äôre using [Markdown formatting.](https://www.markdownguide.org/cheat-sheet/)) #####
<h6> (üí° TIP 2: GitHub Markdown supports <a href="https://gist.github.com/seanh/13a93686bf4c2cb16e658b3cf96807f2"> <em>HTML formatting</em> too, including emojis üòÑ</a>, in case that helps!) </h6>

### :star: Whatever you write in your **README.md** will show up on the ‚Äúfront page‚Äù of your GitHub repo. This is where we‚Äôll be looking for your [weekly progress reports](https://github.com/Berkeley-MDes/24f-desinv-202/wiki/3.0-Weekly-Submissions#weekly-progress-report). They might look something like this: ###


---


# Week of 09/05/2024

This week, I learned how to use a laser cutting printer and printed a tea coaster for trial.

### Reflection

It takes multiple attempts to see the proper red outlines to cut through.

- First Attempt: I put the line stroke as 0.01, and it didn't show up. The other student told me to change it to 0.001, but it still didn't appear.
- Second Attempt: To figure out what the problem is, I just drew a simple square colored 255,0,0 and stroked 0.001px. Then the outline appeared. The problem is the pattern I made.
Third Attempt: I thought the problem was that the pattern layer and the outline overlapped, so I decreased the size of the pattern so that it did not overlap, and it still didn't work
- Third Attempt: I rasterized and expanded the pattern layer before, but I didn't expand it as a vector after masking it. So I made it sure to be fully vectorized. Then it finally worked.

### Speculations

It was so fun to try new equipment that I had never used before. This time, I focused on learning how to use it, which took more time than I expected. I would love to test other cutting settings like etching and more complex features.

### Images & Video
![IMG_6273](https://github.com/user-attachments/assets/358bf897-2883-4a4f-a6fd-049908b7d7cf)
![IMG_6280](https://github.com/user-attachments/assets/e4662d84-3d25-4794-8ad5-92c845167b1d)



### Drawing
<img width="588" alt="Screenshot 2024-09-05 at 1 56 02‚ÄØPM" src="https://github.com/user-attachments/assets/3f4bd015-692b-4d6c-831e-7734a5a8b35d">
<img width="395" alt="Screenshot 2024-09-05 at 12 57 03‚ÄØPM" src="https://github.com/user-attachments/assets/26d23bf2-ddc0-4368-8c40-59adcf624d02">


----

# Week of 09/12/2024
This week, I explored Rhino and Grasshopper and developed some ideas for my computational design project.

## Reflection
I could understand the role of Grasshopper and how the algorithm works.
However, I still don't fully grasp the types of algorithms and how they affect the models.
Currently, I‚Äôm teaching myslef Blender, and I made a donut model to try printing in 3d. I'm going to try printing it in 3D next week.

Also, I started thinking of the object for project 1. Since I tried laser cutting last week, I am excited to try it again. Based on last week's reflection, I will make a helpful, structural object.

## Speculations
(about grasshopper)
I felt lost while exploring the features, and it's because I didn't have a certain goal (project) that I aimed to create. Next time, I will set a goal and plan to make it, then it could be much more feasible to learn this program.

(about laser cutting)
I made some diagrams aligned with the user's context and pain points, including plans to make the object.
I decided to make a headphone stand reflecting my daily life as a user.

<img width="1318" alt="Screenshot 2024-09-19 at 10 28 39‚ÄØAM" src="https://github.com/user-attachments/assets/7be18a29-d905-4589-abd5-2525b4271c2c">



## Images & Video
<img width="552" alt="Screenshot 2024-09-12 at 3 06 16‚ÄØPM" src="https://github.com/user-attachments/assets/a7a127dc-c130-45b0-8603-f7db16459e70">
<img width="1420" alt="Screenshot 2024-09-12 at 3 02 17‚ÄØPM" src="https://github.com/user-attachments/assets/7abda7d5-c397-4a71-b1b3-8a95a33e72e0">


## Sketch
![IMG_6671](https://github.com/user-attachments/assets/6807338c-b783-48cb-8157-4a48548f40f5)



----

# Week of 09/19/2024
I made a final prototype of my headphone stand and tried out 3d printing.
Also, I completed a deliverable video for my cohorts showcasing my journey with this project.

## Reflection

This week, I put in a lot of work to meet my project submission deadline. 
First, I transferred the headphone stand sketch from last week into Adobe Illustrator and laser-cut it. Installing it at home, I was pleased with the result. However, realizing that it also couldn't keep my sunglasses in place, I added a sunglasses hanger and remade the stand.

I recorded a lot of video footage of this process to share in class and edited it using Adobe Premiere Pro.

## Speculations

This week‚Äôs project work, from designing a headphone stand to adapting it into a multifunctional piece, 
focused on the essence of practical design. 

Initially, the stand was meant only for headphones, but adding a sunglasses hanger revealed how each iteration responds to new needs. 
Sharing the process through video deepened my understanding, turning a simple project into a dynamic exploration of form and function. 
It makes me wonder‚Äîcould this be the start of more versatile designs that adapt to everyday life in unexpected ways?

## Images & Video


https://github.com/user-attachments/assets/4d9697f5-d648-49bc-8821-83c5a9e566b2



![IMG_6637](https://github.com/user-attachments/assets/76333a33-2e05-4be2-b6b5-0b4ac0de78a7)

![IMG_6645](https://github.com/user-attachments/assets/6616b3f1-1d54-438e-9916-fe658a260ab1)


https://github.com/user-attachments/assets/7a416543-56bb-4722-b74d-542a14965a35
<img width="1408" alt="Screenshot 2024-09-19 at 1 12 18‚ÄØPM" src="https://github.com/user-attachments/assets/067f4a06-989b-4f60-aced-109811b1f5f8">


## Sketch
<img width="507" alt="Screenshot 2024-09-19 at 1 14 27‚ÄØPM" src="https://github.com/user-attachments/assets/619dd681-263f-49bc-8e4b-d9a228b8c3f0">
<img width="540" alt="Screenshot 2024-09-19 at 1 14 39‚ÄØPM" src="https://github.com/user-attachments/assets/c9c1defb-bbd1-4a7b-be19-10f271ba0d39">


------

# Week of 09/26/2024

## Mapping Interaction Ecosystems

### Entertainment & Media Ecosystems

**First,** I checked my screen time to see which apps I use frequently and to understand their categories.

![IMG_6741](https://github.com/user-attachments/assets/36c8c0f3-a36f-4520-bb97-35855c08e0fc)

**Next**, I considered the purpose of my app usage and how I use them, dividing these two factors into the x and y axes to create a positioning map.
- X: Informational <-> Entertainment
- Y: Social (connecting with other people) <-> Content Consuming (not connecting other people, just consuming other's content)

<img width="1316" alt="Screenshot 2024-09-26 at 1 12 24‚ÄØPM" src="https://github.com/user-attachments/assets/bc193914-ee5a-45df-bcad-8e8331ab993e">


**Last**, I reclassified the purposes of my app usage once more. I identified that the flow of obtaining, consuming information, and interacting with others is connected linearly, and I organized how this flow operates.

1. I obtain official information about school and job-related matters through Slack, WhatsApp cohort group chats, and LinkedIn. I mainly get information about official events from the school or companies, internship opportunities, exhibitions, and competitions.

2. If the information from step 1 is insufficient or if I need others' opinions or additional comments on that information, I turn to Reddit for further insights. At this stage, extra information is added to the formal information obtained earlier, creating a flow where the overall amount of information increases.

3. I gather a mix of official and unofficial information through platforms like X (formerly Twitter), TikTok, and YouTube. This information is often entertainment-oriented, helping me relieve stress or stay updated on social trends.

4. I process the information obtained from steps 1, 2, and 3 and engage in conversations with others, using this as a means of social networking. Others also go through similar steps, and there‚Äôs a flow where we exchange the information we've gathered from these processes.


<img width="1017" alt="Screenshot 2024-09-26 at 1 14 32‚ÄØPM" src="https://github.com/user-attachments/assets/12d5183c-29f4-4ffe-acbc-75ace387f151">

-----

# Week of 2024/10/03

This week, I installed the platform called Particle and learned the basics of C++ using the Photon. Since it was my first time using VSCode, it took me a while to get familiar with the interface, but I‚Äôm proud to have successfully completed the basic setup and monitoring.

However, my progress has been slower than Jeff's expectations, so I plan to work harder next week to catch up.

### Process

1. 9/26 in class: I wrote code where the entire "hello world!" sentence appears at 3-second intervals.
2. 9/30 homework: I wrote code where each character of "hello world!" appears at 3-second intervals.
 ```
   void loop() {
  if (count == size_hello) {
    count = 0;
  }

  // Test with simpler log output to ensure logging works correctly
  Log.info("current character: %c", hello[count]);
  
  count++;
  delay(300);
}
```

https://github.com/user-attachments/assets/1dbbd4d1-1987-471a-a48b-7492cd9b260f


3. Next, I adjusted settings like the class name and delay, and instead of the phrase "hello world!", I made each character of my name appear one by one.

 ```
#include "Particle.h"

// Define the system setup and log handler
SYSTEM_MODE(AUTOMATIC);
SYSTEM_THREAD(ENABLED);
SerialLogHandler logHandler(LOG_LEVEL_ALL);  // Î°úÍ∑∏ Î†àÎ≤®ÏùÑ ALLÎ°ú ÏÑ§Ï†ï

char myname[] = "Jeongmin Lee ";
int size_myname = sizeof(myname) - 1;
int count = 0;

void setup() {
  Log.info("-------------Welcome to the myname World example!-------------");
}

void loop() {
  if (count == size_myname) {
    count = 0;
  }

  // Test with simpler log output to ensure logging works correctly
  Log.info("current character: %c", myname[count]);
  
  count++;
  delay(300);
}

 ```

https://github.com/user-attachments/assets/375dc102-5986-487c-80de-9db439cb7d93


### Failure
For some unknown reason, I opened the example file that Jeff provided and started monitoring, but despite setting everything up correctly, the monitoring kept failing. When I copied and pasted the code into the file I was originally working on and started monitoring, it worked fine, but I couldn't figure out why.



### Reflection

Though the progress is very slow, I‚Äôm grateful to be learning bit by bit. By next week, I will ensure that I can monitor the display on Photon. Additionally, I‚Äôll challenge myself to try out the example Jeff explained, where the LED blinks using a button.


### Speculation

What could I create with what I learned today?

I could make something that outputs words of encouragement or support at specific times. For example, when feeling tired from working for long periods, it could show uplifting words every two hours. Or, during a discussion, when I feel hesitant to speak up, it could display a message to encourage me to share my thoughts.

Small words can easily influence people's moods and attitudes. The text display I learned to create this week could serve this purpose.


-----
# Week of 2024/10/10


This experiment explores sensor-based projects using the Stemma QT interface with a Particle device. It involves soldering the interface, configuring the project in Visual Studio Code, and experimenting with the demo firmware. The objective is to map sensor values to outputs like LED control and share data between devices using Particle.publish() and Particle.subscribe(). This report summarizes the findings and considerations.


### Soldering
![IMG_6949](https://github.com/user-attachments/assets/f1afd0aa-b629-4541-abe8-dbf5eb11d0d3)
![IMG_6950](https://github.com/user-attachments/assets/d8065872-fe6f-4f03-bc8d-187215a20c26)

Josh helped me with how to do soldering and instructed me to use it safely.

### Reflection
The experiment showed the potential of integrating sensor data with various outputs and sharing sensor values across devices. While the basic demo firmware provided limited functionality, through the use of map(), constrain(), and data smoothing techniques, sensor values could be mapped to other processes such as LED brightness control. The feasibility of sharing sensor data via Particle‚Äôs cloud was explored, highlighting both opportunities and challenges in latency and data accuracy. Further work could involve expanding the output options and testing real-time data synchronization across multiple devices.

### Speculation
Based on this experiment, Subin, Hannah, and I grouped as a team for the 2nd project.
Our concept is about a calming down system for people who are arguing/conflicting.
We are going to use sound input and recognize the sound, whether it is fighting or not.
Then, the music sound will be used as an output.
Using a pulse sensor would indicate calmness.



### Diagram: Design and Data Flow
The following diagram represents the system design and data flow:

 ```
[ Stemma QT Sensors ] --> [ Particle Device ]
        |                    |
        V                    V
[ Sensor Data Mapping ]    [ Publish Sensor Data to Cloud ]
        |                    |
        V                    V
[ LED Control ]         [ Subscribe to Data on Second Device ]
                             |
                             V
                 [ Second Device Uses Data for Output ]

 ```

-----

# Week of 2024/10/17

This week, our team developed each own part. My part is making a physical artifact for containing a speaker, mic and photon.

## Process

### for frame
1. I designed a simple box-shaped product with 2 feet (10x10x4cm) to put all things in it.
2. Also, I made a little hole to put the wires out.

### for lids
Then, I designed the front and back lids. 
1. The back is just a simple panel,
2. but the front lid should have a hole for the mic, which is a 1cm hole.
3. Also, I made lined holes that make sound come out.

### Frame + lids
I made 4.5mm holes on each edge of the frame and added 4 extrudes on the front and back lids to make them fit each other.


## Sketch
![IMG_7261](https://github.com/user-attachments/assets/a9b734bf-1d90-4f0e-a910-110f29e759a3)


## Image
![IMG_7263](https://github.com/user-attachments/assets/447f19f8-5c5c-4f8d-80b4-c4c69325d527)



https://github.com/user-attachments/assets/d1685436-6a55-4642-8f3b-1ec49e268724




## Reflection
I initially sketched the ultrasonic device to look like a face with eyes, nose, and mouth, thinking it was a speaker. However, I realized it wasn't a speaker and, with Jeff's help, I was able to get a larger speaker. Although it's a bit big, it fortunately fits. Over the weekend, I plan to shoot the demo video and edit the final version.


-----
# Week of 2024/10/24

## Reflection

This week, my team collaborated to film and edit a demonstration video. We encountered challenges connecting the Photon to the Berkeley IoT Wi-Fi, which led to delays as we worked to establish a proper connection. I eventually resolved the issue by identifying alternative networks. I directed the demo video, with Subin and Hannah taking on acting roles. After filming, I gathered everyone's footage and edited the video.

## Speculation

Moving forward, I anticipate that our team will become more skilled at addressing IoT connectivity challenges, leading to quicker setups during filming. To prevent similar setbacks, we could create a checklist specifically for connecting the Photon and Berkeley IoT Wi-Fi, ensuring a smoother workflow in future projects.

In terms of video production, I see opportunities to improve our process by planning transitions and roles more thoroughly before filming. This will help make the editing process more efficient. Collaborating on this project has strengthened our teamwork, and I‚Äôm optimistic that our communication and coordination will continue to improve as we take on more complex tasks.


## Video

https://youtu.be/Mkrjj4jm3fM

this is our final video

## Sketch
There was no sketch for this week.

- [TDF Wiki](https://github.com/Berkeley-MDes/24f-desinv-202/wiki) - the ultimate source for truth and information about the course and assignments
- [Google Drive Folder](https://drive.google.com/drive/u/0/folders/1DJ1b6sSDwHXX6NRcQYt10ivyQSgU0ND6) - slides and other resources
- [bCourses](https://bcourses.berkeley.edu/courses/1537533) - where the grading happens


# Week of 2024/10/31

## Process

I did 4 different experiments on Zerowidth

1. Experiment 1: GPT + Temperature
   I asked "What is your purpose?" 
<img width="1485" alt="Screenshot 2024-10-29 at 11 22 17‚ÄØPM" src="https://github.com/user-attachments/assets/41fbfa31-966a-4e38-8114-e54e6e8fd878">

then adjusted temperature
<img width="1460" alt="Screenshot 2024-10-29 at 11 24 55‚ÄØPM" src="https://github.com/user-attachments/assets/671c5d69-d3bb-4da6-8b04-b34b7ee86da0">

Errors occurred sometimes.

<img width="1530" alt="Screenshot 2024-10-29 at 11 25 21‚ÄØPM" src="https://github.com/user-attachments/assets/d8c094cc-97f6-4b6b-a64d-6416be75402c">

It's alright now. I could see the answer changed when I adjusted the slider, but I didn't see any functional values changed.


2. Experiment 2: GPT + Instruct prompt
<img width="1278" alt="Screenshot 2024-10-29 at 11 42 48‚ÄØPM" src="https://github.com/user-attachments/assets/5cfcce17-21a6-43da-82a7-98a785809ba0">

I added an instruct prompt. Then, its answer got more relevant to my information. But I think it's a kind of manual approach.


3. Experiment 3: GPT + Instruct + RAG

<img width="1530" alt="Screenshot 2024-10-29 at 11 56 46‚ÄØPM" src="https://github.com/user-attachments/assets/ab37b8a0-0b0c-4c6c-b0bc-485f84fbe476">

I created a knowledge base based on my CV, and it created 10 chunks since I set 55 initial chunks
In TJ's tutorial, he set 300 initial chunks, which had 11 chunks.
I think that's the reason why my LLM's answer is not accurate?


4. Experiment 4: GPT + Instruct + RAG +Variables

I added 2 variables: Location and Year.
I didn't really understand how it related to the answer result. I'll figure it out during the next class.

<img width="1426" alt="Screenshot 2024-10-30 at 12 21 52‚ÄØAM" src="https://github.com/user-attachments/assets/4eccc87e-25c1-4534-ac8f-46b25f3df1b6">

## Reflection

I think most of the correct answers are based on my manually written instruction prompt.
When I ask random questions that are not covered through the prompt (e.g., where was she during 2022?)
It created hallucinations.

## Speculation

I can imagine I would practice job interviews by using Mini Me. It understood my background and experience and created reasonable answers.
So, I might crawl the expected interview questions and ask them. Then, I would get outlines for my interview answers.



------


# Week of 2024/11/07

## Process

Last week, even though I included the knowledge base, the response results were unsatisfactory. I found the reason: I hadn‚Äôt included a sentence in the prompt to retrieve the user query. After fixing this, I achieved satisfactory results.

For my final experiment, I added 2 more knowledge bases: MDes class curriculum and my TDF weekly report.
I asked 5 questions and the result was great and made sense.
<img width="967" alt="Screenshot 2024-11-07 at 12 48 06‚ÄØPM" src="https://github.com/user-attachments/assets/da716b68-6d8a-40f4-80b8-f5ca23029289">


## Reflection
after I finished the final experiment I asked the questions for evaluation and the answer was pretty accurate and natural. 
It was fun to see the result especially when it said i felt being lost while exploring the features of the Grasshopper program,
and when it mentioned about my headphone stand for my first project.

I satisfied with the result and it was fun process to learn about LLM following the video tutorials.

## Speculation

- human experience
As personalized AIs become more integrated, human interaction with technology could shift from static to adaptive, tailored experiences. This could reshape the Anthropogenic Environment by making it responsive to cultural identity and diversity, pushing AI to recognize and respect individual backgrounds. Culturally, this may drive new expectations for AI to accommodate personal and cultural nuances within shared digital spaces.

- Engineering
Personalized AI could lead to expectations for products to be customizable and responsive, with engineering increasingly focused on modularity and adaptability. This shift would require engineers to consider cultural diversity and ethical factors like privacy, ensuring products serve users individually while respecting cultural contexts.

- The Role of AI in AI Development
AI could help refine and improve other AI systems, especially in understanding diverse user backgrounds and detecting bias. By enabling recursive improvement, AI might evolve to better reflect human cultural diversity and adapt to shifting societal needs, co-evolving alongside human expectations.

-----


# Week of 2024/11/14

## Process

This week, I completed my video presentation, marking the end of a major phase in my current project. Preparing for this presentation involved consolidating all the work I‚Äôve done so far, from initial experiments to refining my final system design. It was a valuable opportunity to review the progress made, evaluate my understanding, and reflect on areas where I could improve or expand my approach. Completing the presentation felt rewarding, as it allowed me to share the results of my work, demonstrating the knowledge and skills I‚Äôve developed over the semester.

## Reflection
Now that the presentation is complete, I have started to shift my focus towards planning a final project. This transition feels timely, as the progress I‚Äôve made in recent weeks has given me a solid foundation of technical skills, which I‚Äôm eager to expand upon. Thinking about the final project is exciting because it presents an opportunity to bring together everything I‚Äôve learned and to explore a more comprehensive, ambitious application of my skills. I‚Äôm considering different directions I could take, such as creating an interactive installation or a tool that enhances user engagement through adaptive design.

One area I‚Äôm particularly interested in is integrating sensor-based technology with AI-driven responses to create an experience that feels responsive and personalized. Another possibility is to focus on user-centered design principles, using my knowledge of interaction design to create a solution that addresses specific needs or problems. As I brainstorm, I‚Äôm keeping in mind the feedback I‚Äôve received throughout the semester, especially regarding adaptability and user engagement, to guide my project‚Äôs direction.

Overall, this week has been both a period of reflection and a time for forward planning. I‚Äôm looking forward to developing a project that challenges me creatively and technically, leveraging the skills I‚Äôve gained this semester to create something impactful.

-----

# Week of 2024/11/21

### Reflection
This week, I successfully integrated the Photon camera with cloud storage and validated the feasibility of pixelating images for simplified visual representation. The process reinforced the importance of aligning hardware functionality with user interaction goals, ensuring a seamless experience.

### Speculation
Moving forward, I anticipate challenges in fine-tuning the button‚Äôs press duration feature to capture happiness intensity accurately. Exploring user testing will be key to refining this feature and ensuring it aligns with real-world emotional expressions.

### Image
![Screenshot 2024-11-22 at 2 16 09‚ÄØPM](https://github.com/user-attachments/assets/8cc1edd5-10e2-428a-a73c-092f5c6b6e03)
This is final UI for app screen

### Sketch
![Screenshot 2024-11-22 at 2 16 31‚ÄØPM](https://github.com/user-attachments/assets/d1919061-2cdb-49b7-8da1-d0903ffa9765)
This is first draft of this concept
